{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a70e0c5-d242-4242-90f3-f63094fa91b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Invert predictions and actual values to original scale\n",
    "test_predictions = scaler.inverse_transform(test_predictions)\n",
    "y_test = scaler.inverse_transform([y_test])\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test[0], test_predictions[:, 0])\n",
    "rmse = np.sqrt(mean_squared_error(y_test[0], test_predictions[:, 0]))\n",
    "print('Mean Absolute Error (MAE):', mae)\n",
    "print('Root Mean Squared Error (RMSE):', rmse)\n",
    "\n",
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test[0], label='Actual')\n",
    "plt.plot(test_predictions[:, 0], label='Predicted')\n",
    "plt.title('Test Set Predictions vs Actual Values')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Temperature')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5930bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define a function to create the LSTM model\n",
    "def create_lstm_model(learning_rate=0.001, units=50, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=units, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units=units, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units=units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units=1))\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Wrap the Keras model in a scikit-learn estimator\n",
    "estimator = KerasRegressor(build_fn=create_lstm_model)\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'units': [50, 100],\n",
    "    'dropout_rate': [0.2, 0.3]\n",
    "}\n",
    "\n",
    "# Perform grid search with 3-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_result = grid_search.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, verbose=0)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding mean squared error\n",
    "print(\"Best Hyperparameters:\", grid_result.best_params_)\n",
    "print(\"Best Mean Squared Error:\", -grid_result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\"\n",
    "data = pd.read_csv(url, parse_dates=['Date'], index_col='Date')\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Define function to create dataset with input and output sequences\n",
    "def create_dataset(data, time_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:(i + time_steps), 0])\n",
    "        y.append(data[i + time_steps, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Set time steps and split into train and test sets\n",
    "time_steps = 30\n",
    "X, y = create_dataset(data_scaled, time_steps)\n",
    "X_train, X_test = X[:int(X.shape[0]*0.8)], X[int(X.shape[0]*0.8):]\n",
    "y_train, y_test = y[:int(y.shape[0]*0.8)], y[int(y.shape[0]*0.8):]\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Plot training loss and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = model.predict(X_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Invert predictions to original scale\n",
    "train_predictions = scaler.inverse_transform(train_predictions)\n",
    "y_train = scaler.inverse_transform([y_train])\n",
    "test_predictions = scaler.inverse_transform(test_predictions)\n",
    "y_test = scaler.inverse_transform([y_test])\n",
    "\n",
    "# Calculate RMSE\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train[0], train_predictions[:,0]))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test[0], test_predictions[:,0]))\n",
    "print('Train RMSE:', train_rmse)\n",
    "print('Test RMSE:', test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\"\n",
    "data = pd.read_csv(url, parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# Preprocessing\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Splitting into training and test sets\n",
    "train_size = int(len(data_scaled) * 0.8)  # 80% for training\n",
    "test_size = len(data_scaled) - train_size\n",
    "train, test = data_scaled[0:train_size,:], data_scaled[train_size:len(data_scaled),:]\n",
    "\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Test set size:\", len(test))\n",
    "\n",
    "# Visualize the original and normalized data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data.index, data['Temp'], label='Original Data')\n",
    "plt.title('Daily Minimum Temperatures in Melbourne')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature (Â°C)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78554d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f753c45-9152-4233-bd35-10ac08c867e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad859b-dfd4-422f-a0a1-e2a5d5a524ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of epochs\n",
    "epochs = 50\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=64, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    shuffle=True)\n",
    "\n",
    "# Plot training/validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16345788-bb52-4b8b-bf6b-e8cafb0cbfc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926b04b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c32c2-e14b-4185-b174-0888e8aee091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Invert predictions and actual values to original scale\n",
    "test_predictions = scaler.inverse_transform(test_predictions)\n",
    "y_test = scaler.inverse_transform([y_test])\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test[0], test_predictions[:, 0])\n",
    "rmse = np.sqrt(mean_squared_error(y_test[0], test_predictions[:, 0]))\n",
    "print('Mean Absolute Error (MAE):', mae)\n",
    "print('Root Mean Squared Error (RMSE):', rmse)\n",
    "\n",
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test[0], label='Actual')\n",
    "plt.plot(test_predictions[:, 0], label='Predicted')\n",
    "plt.title('Test Set Predictions vs Actual Values')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Temperature')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Challenges Encountered during Model Training and Optimization:\n",
    "\n",
    "One common challenge is overfitting, especially when dealing with complex LSTM architectures and limited training data. This can be mitigated through techniques like dropout and regularization.\n",
    "Another challenge is finding the right balance between model complexity and computational resources. Increasing the number of LSTM layers or units can improve model performance, but it also increases training time and resource requirements.\n",
    "Decision on Number of LSTM Layers and Units:\n",
    "\n",
    "The decision on the number of LSTM layers and units depends on the complexity of the data and the modeling task. It often involves a trade-off between model complexity and computational resources.\n",
    "Generally, starting with a simpler model and gradually increasing complexity while monitoring performance on a validation set is a common approach.\n",
    "Preprocessing Steps on Time Series Data:\n",
    "\n",
    "Preprocessing steps typically include handling missing values, normalization to bring all features to a similar scale, and splitting the data into training and test sets while preserving temporal order.\n",
    "Additionally, feature engineering may involve creating lag features, extracting trend and seasonality, or encoding categorical variables.\n",
    "Purpose of Dropout Layers in LSTM Networks:\n",
    "\n",
    "Dropout layers are used in LSTM networks to prevent overfitting by randomly dropping a proportion of neurons during training. This prevents neurons from relying too heavily on specific inputs and encourages the network to learn more robust features.\n",
    "Dropout helps in improving generalization by reducing the model's reliance on any single feature or combination of features, thus making the network less sensitive to noise in the training data.\n",
    "Model's Ability to Capture Long-Term Dependencies and Make Accurate Predictions:\n",
    "\n",
    "LSTM networks are specifically designed to capture long-term dependencies in sequential data. Their ability to remember information over extended time periods makes them well-suited for time series forecasting tasks.\n",
    "The accuracy of predictions depends on various factors including the quality and quantity of data, the chosen architecture, hyperparameters, and training methodology.\n",
    "Potential Improvements or Alternative Approaches:\n",
    "\n",
    "Experimenting with different architectures, such as adding more LSTM layers or units, using different activation functions, or incorporating attention mechanisms, could potentially improve performance.\n",
    "Ensemble methods, such as combining predictions from multiple models or incorporating external data sources, may also enhance forecasting accuracy.\n",
    "Bayesian optimization or reinforcement learning-based approaches could be explored for more efficient hyperparameter tuning.\n",
    "Additionally, considering alternative models such as CNNs or Transformer architectures for time series forecasting tasks could offer valuable insights.\n",
    "\n",
    "User"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
